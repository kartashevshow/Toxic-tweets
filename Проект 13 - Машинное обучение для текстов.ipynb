{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выводы-по-предобработке:\" data-toc-modified-id=\"Выводы-по-предобработке:-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Выводы по предобработке:</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Случайный-лес\" data-toc-modified-id=\"Случайный-лес-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Случайный лес</a></span></li><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Тестирование-лучшей-модели\" data-toc-modified-id=\"Тестирование-лучшей-модели-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Тестирование лучшей модели</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "m = Mystem()\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проверим количество дубликатов\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проверим соотношение классов\n",
    "df.value_counts('toxic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдается ясный перекос классов. Для того, чтобы получить более высокое значение F1-меры, выполним взвешивание классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Очистим текст от лишних символов\n",
    "def cleaning(text):\n",
    "    text = re.sub(r\"(?:\\n|\\r)\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z ]+\", \"\", text).strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "def lemmatize(text): \n",
    "    return \" \".join([wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "                     for word, tag in pos_tag(word_tokenize(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [09:11<00:00, 288.70it/s]\n"
     ]
    }
   ],
   "source": [
    "df['lemm_text'] = df['text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, выполняющая лемматизацию работает очень долго. Не понимаю как выполнить ее, не потратив 2 дня на это. Для того, чтобы выполнить проект с BERT, необходим файл со словарем, как я понял из темы. Файла приложено не было. Так что оставляю так как есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы по предобработке:\n",
    "Пропусков не обнаружено;\n",
    "\n",
    "Дубликатов не обнаружено;\n",
    "\n",
    "Наблюдается явный перекос классов. Нужно будет учесть это при обучении модели;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разделим модели на тренировочную и тестовую выборки. Результат будем проверять на кросс - валидации.\n",
    "features = df['lemm_text']\n",
    "target = df['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, shuffle=False, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Превратим текст в векторный вид\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tfidf_train = count_tf_idf.fit_transform(features_train)\n",
    "tfidf_test = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код ниже лучше не запускать, иначе можно потерять 40 минут времени. Почему - то лучший счет в содели случайного леса - 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "F1 score: 0.32472834356332253\n",
      "CPU times: total: 21.3 s\n",
      "Wall time: 50min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params_rf = {\n",
    "    'n_estimators': list(range(50,300,50)),\n",
    "    'max_depth':[5,15],\n",
    "    'max_features' : list(range(1,20, 2))\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(class_weight = 'balanced',  \n",
    "                            random_state=42)\n",
    "                                 \n",
    "grid_rf = GridSearchCV(rf, param_grid=params_rf, scoring='f1', cv=3, verbose=True, n_jobs=-1)\n",
    "best_grid = grid_rf.fit(tfidf_train, target_train)\n",
    "print('F1 score:', grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "F1 score: 0.7587194472562331\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C': list(range(1,15,3))\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(class_weight = 'balanced', \n",
    "                        random_state=42,\n",
    "                        max_iter=200)\n",
    "\n",
    "grid_lr = GridSearchCV(lr, param_grid=param_grid, scoring='f1', cv=3, verbose=True, n_jobs=-1)\n",
    "best_grid = grid_lr.fit(tfidf_train, target_train)\n",
    "print('F1 score:', grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7604456824512537"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(\n",
    "    penalty = grid_lr.best_params_['penalty'],\n",
    "    C = grid_lr.best_params_['C'],\n",
    "    max_iter=200,\n",
    "    class_weight = 'balanced',\n",
    "    random_state=42)\n",
    "\n",
    "lr.fit(tfidf_train, target_train)\n",
    "\n",
    "f1_score(target_test, lr.predict(tfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении моделей был учтен дисбаланс классов. Из рассмотренных моделей, лучшее качество показала логистическая регрессия.  Значение F1 меры на тестовой выборке удовлетворяет условиям задачи. "
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 58,
    "start_time": "2023-05-16T14:26:38.798Z"
   },
   {
    "duration": 413,
    "start_time": "2023-05-16T14:26:54.603Z"
   },
   {
    "duration": 32,
    "start_time": "2023-05-16T14:26:56.799Z"
   },
   {
    "duration": 2963,
    "start_time": "2023-05-16T14:27:08.362Z"
   },
   {
    "duration": 39,
    "start_time": "2023-05-16T14:27:22.314Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-16T14:28:02.866Z"
   },
   {
    "duration": 333,
    "start_time": "2023-05-16T14:28:28.162Z"
   },
   {
    "duration": 249,
    "start_time": "2023-05-16T14:28:33.143Z"
   },
   {
    "duration": 1486,
    "start_time": "2023-05-16T14:37:30.955Z"
   },
   {
    "duration": 86,
    "start_time": "2023-05-16T14:38:37.777Z"
   },
   {
    "duration": 22,
    "start_time": "2023-05-16T14:39:23.157Z"
   },
   {
    "duration": 19,
    "start_time": "2023-05-16T14:39:32.114Z"
   },
   {
    "duration": 535,
    "start_time": "2023-05-16T14:41:40.949Z"
   },
   {
    "duration": 426,
    "start_time": "2023-05-16T14:41:55.754Z"
   },
   {
    "duration": 587,
    "start_time": "2023-05-16T14:42:49.090Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-16T14:43:26.369Z"
   },
   {
    "duration": 504,
    "start_time": "2023-05-16T14:43:29.569Z"
   },
   {
    "duration": 1144,
    "start_time": "2023-05-16T14:44:13.752Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-16T14:44:19.305Z"
   },
   {
    "duration": 1068,
    "start_time": "2023-05-16T14:44:22.246Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-16T14:46:06.310Z"
   },
   {
    "duration": 2647,
    "start_time": "2023-05-19T17:13:55.014Z"
   },
   {
    "duration": 3025,
    "start_time": "2023-05-19T17:14:04.858Z"
   },
   {
    "duration": 3022,
    "start_time": "2023-05-19T17:14:13.688Z"
   },
   {
    "duration": 2903,
    "start_time": "2023-05-19T17:14:19.644Z"
   },
   {
    "duration": 48,
    "start_time": "2023-05-19T17:15:09.218Z"
   },
   {
    "duration": 149,
    "start_time": "2023-05-19T17:19:36.361Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-19T17:19:43.257Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-19T17:19:48.801Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-19T17:23:06.215Z"
   },
   {
    "duration": 49,
    "start_time": "2023-05-19T17:23:11.640Z"
   },
   {
    "duration": 1456,
    "start_time": "2023-05-19T17:23:30.540Z"
   },
   {
    "duration": 1474,
    "start_time": "2023-05-19T17:23:38.238Z"
   },
   {
    "duration": 57,
    "start_time": "2023-05-19T17:24:53.538Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-19T17:25:12.604Z"
   },
   {
    "duration": 18,
    "start_time": "2023-05-19T17:26:20.586Z"
   },
   {
    "duration": 7,
    "start_time": "2023-05-19T17:27:25.540Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-19T17:27:52.600Z"
   },
   {
    "duration": 15,
    "start_time": "2023-05-19T17:28:20.943Z"
   },
   {
    "duration": 922,
    "start_time": "2023-05-19T17:28:28.746Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-19T17:28:37.244Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-19T17:29:20.695Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-19T17:30:09.746Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-19T17:30:26.159Z"
   },
   {
    "duration": 551774,
    "start_time": "2023-05-19T17:30:29.292Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-19T17:44:23.405Z"
   },
   {
    "duration": 28,
    "start_time": "2023-05-19T17:45:34.314Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-19T17:45:51.088Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-19T17:45:56.137Z"
   },
   {
    "duration": 6971,
    "start_time": "2023-05-19T17:45:59.208Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-19T17:53:01.489Z"
   },
   {
    "duration": 1444759,
    "start_time": "2023-05-19T17:53:06.967Z"
   },
   {
    "duration": 42,
    "start_time": "2023-05-19T18:22:26.806Z"
   },
   {
    "duration": 19,
    "start_time": "2023-05-19T18:22:32.055Z"
   },
   {
    "duration": 108688,
    "start_time": "2023-05-19T18:23:01.582Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-19T18:24:50.378Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
